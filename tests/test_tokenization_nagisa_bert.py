from nagisa_bert.tokenization_nagisa_bert import NagisaBertTokenizer


class TestNagisaBertTokenizer:

    tokenizer = NagisaBertTokenizer.from_pretrained("taishi-i/nagisa_bert")

    text = "nagisaã§[MASK]ã§ãã‚‹BERTãƒ¢ãƒ‡ãƒ«ã§ã™"

    def test_vocab_size(self):
        expected_vocab_size = 32000
        assert expected_vocab_size == self.tokenizer.vocab_size

    def test_get_vocab(self):
        expected_vocab_size = 32000
        expected_internet_token_id = 10171

        assert expected_vocab_size == len(self.tokenizer.get_vocab())
        assert expected_internet_token_id == self.tokenizer.get_vocab()["ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆ"]

    def test_pad_token(self):
        expected_pad_token = "[PAD]"
        expected_pad_token_id = 0

        assert expected_pad_token == self.tokenizer.pad_token
        assert expected_pad_token_id == self.tokenizer.pad_token_id

    def test_unk_token(self):
        expected_unk_token = "[UNK]"
        expected_unk_token_id = 1

        assert expected_unk_token == self.tokenizer.unk_token
        assert expected_unk_token_id == self.tokenizer.unk_token_id

    def test_cls_token(self):
        expected_cls_token = "[CLS]"
        expected_cls_token_id = 2

        assert expected_cls_token == self.tokenizer.cls_token
        assert expected_cls_token_id == self.tokenizer.cls_token_id

    def test_sep_token(self):
        expected_sep_token = "[SEP]"
        expected_sep_token_id = 3

        assert expected_sep_token == self.tokenizer.sep_token
        assert expected_sep_token_id == self.tokenizer.sep_token_id

    def test_mask_token(self):
        expected_mask_token = "[MASK]"
        expected_mask_token_id = 4

        assert expected_mask_token == self.tokenizer.mask_token
        assert expected_mask_token_id == self.tokenizer.mask_token_id

    def test_tokenize_basic(self):
        expected_tokens = [
            "na",
            "##g",
            "##is",
            "##a",
            "ã§",
            "[MASK]",
            "ã§ãã‚‹",
            "BE",
            "##R",
            "##T",
            "ãƒ¢ãƒ‡ãƒ«",
            "ã§ã™",
        ]

        tokens = self.tokenizer.tokenize(self.text)
        assert expected_tokens == tokens

    def test_tokenize_sample_hankaku_zenkaku(self):
        text = "ï½ºï¾ï¾Šï¾ï¾ï¾Šï¼‘ï¼’ï¼“ï¼”ï¼•"
        expected_tokens = ["ã‚³ãƒ³", "##ãƒãƒ³", "##ãƒ", "1", "2", "3", "4", "5"]

        tokens = self.tokenizer.tokenize(text)
        assert expected_tokens == tokens

    def test_tokenize_sample_emoji_zenkaku(self):
        text = "ã“ã‚“ã°ã‚“ã¯ğŸ˜€ï¼°ï¼¹ï¼´ï¼¨ï¼¯ï¼®"
        expected_tokens = ["ã“ã‚“", "##ã°ã‚“", "##ã¯", "[UNK]", "P", "##Y", "##TH", "##ON"]

        tokens = self.tokenizer.tokenize(text)
        assert expected_tokens == tokens

    def test_tokenize_sample_kaomoji(self):
        text = "(äººâ€¢á´—â€¢â™¡)ã“ã‚“ã°ã‚“ã¯â™ª"
        expected_tokens = ["[UNK]", "ã“ã‚“", "##ã°ã‚“", "##ã¯", "â™ª"]

        tokens = self.tokenizer.tokenize(text)
        assert expected_tokens == tokens

    def test_tokenize_unknown_chars(self):
        text = "ğª—±ğª˜‚ğª˜šğªš²"
        expected_tokens = ["[UNK]", "[UNK]", "[UNK]", "[UNK]"]

        tokens = self.tokenizer.tokenize(text)
        assert expected_tokens == tokens

    def test_encode(self):
        expected_output = {
            "input_ids": [
                2,
                30314,
                4015,
                8143,
                4012,
                451,
                4,
                8097,
                14270,
                4119,
                4121,
                8455,
                8489,
                3,
            ],
            "token_type_ids": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
            "attention_mask": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        }

        output = self.tokenizer(self.text)
        assert expected_output["input_ids"] == output["input_ids"]
        assert expected_output["token_type_ids"] == output["token_type_ids"]
        assert expected_output["attention_mask"] == output["attention_mask"]

    def test_convert_ids_to_tokens(self):
        output = self.tokenizer(self.text)

        expected_tokens = [
            "[CLS]",
            "na",
            "##g",
            "##is",
            "##a",
            "ã§",
            "[MASK]",
            "ã§ãã‚‹",
            "BE",
            "##R",
            "##T",
            "ãƒ¢ãƒ‡ãƒ«",
            "ã§ã™",
            "[SEP]",
        ]

        tokens = self.tokenizer.convert_ids_to_tokens(output["input_ids"])
        assert expected_tokens == tokens
